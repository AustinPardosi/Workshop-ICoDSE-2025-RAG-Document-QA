{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afadfb2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e731bf",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">About this notebook</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77080f",
   "metadata": {},
   "source": [
    "This is a supplementary notebook for the session AG __Studi Kasus: Document Q&A__  of the __ICoDSE 2025__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43120e0f",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Contents</span>\n",
    "\n",
    "This notebook contains code in python and leverages the LangChain framework to build and evaluate the different components of a RAG pipeline. \n",
    "\n",
    "- Indexing Pipeline\n",
    "    -  Data Loading\n",
    "    - Chunking (or Data Splitting)\n",
    "    - Embeddings (or Data Transformation)\n",
    "    - Storage (Vector Databases)\n",
    "\n",
    "- Generation Pipeline\n",
    "    - Search & Retrieval\n",
    "    - Prompt Augmentation\n",
    "    - LLM Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6eaaf1",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">Installing Dependencies</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a40a31",
   "metadata": {},
   "source": [
    "All the necessary libraries for running this notebook along with their versions can be found in __requirements.txt__ file in the root directory of this repository\n",
    "\n",
    "You should go to the root directory and run the following command to install the libraries\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "This is the recommended method of installing the dependencies\n",
    "\n",
    "\n",
    "_Alternatively, you can run the command from this notebook too. The relative path may vary so ensure that you are in the root directory of this repository_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c024484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ./requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c3e0a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d02eb4",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">Indexing Pipeline</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9041f80",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Data Loading</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='./Assets/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c51c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "#START YOUR CODE HERE\n",
    "pdfloader=PyPDFLoader(file_path=filepath, mode='single') #instantiate the PyPDFLoader\n",
    "\n",
    "pdf_data=pdfloader.load() #load the data\n",
    "\n",
    "print(textwrap.fill(f\"{pdf_data[0].page_content[:1000]}\", width=150)) #print the first 1000 characters\n",
    "\n",
    "#END YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cff696",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for solution</summary>\n",
    "\n",
    "```\n",
    "pdfloader=PyPDFLoader(file_path=filepath, mode='single') #instantiate the PyPDFLoader\n",
    "\n",
    "pdf_data=pdfloader.load() #load the data\n",
    "\n",
    "print(textwrap.fill(f\"{pdf_data[0].page_content[:1000]}\", width=150)) #print the first 1000 characters\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c543d276",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Data Splitting or Chunking</span>\n",
    "\n",
    "> Breaking down long pieces of text into manageable sizes is called Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab67df",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Fixed Size Chunking</span>\n",
    "\n",
    "A very common approach is to pre-determine the size of the chunk and the amount of overlap between the chunks. There are several chunking methods that follow a fixed size chunking approach.\n",
    "\n",
    "- Character-Based Chunking: Chunks are created based on a fixed number of characters\n",
    "\n",
    "- Token-Based Chunking: Chunks are created based on a fixed number of tokens.\n",
    "\n",
    "- Sentence-Based Chunking: Chunks are defined by a fixed number of sentences\n",
    "\n",
    "- Paragraph-Based Chunking: Chunks are created by dividing the text into a fixed number of paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827dd29f",
   "metadata": {},
   "source": [
    "Let's try Character-Based Chunking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfloader=PyPDFLoader(file_path=filepath,mode=\"single\") #instantiate the PyPDFLoader\n",
    "\n",
    "pdf_data=pdfloader.load() #load the data\n",
    "\n",
    "print(len(pdf_data[0].page_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#START YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "#END YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87dd5a1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Solution</summary>\n",
    "\n",
    "```\n",
    "text_splitter =RecursiveCharacterTextSplitter(\n",
    "separators=[\"\\n\\n\",\"\\n\",\".\"], #The character that should be used to split. More than one can be given to try recursively.\n",
    "chunk_size=1000, #Number of characters in each chunk \n",
    "chunk_overlap=100, #Number of overlapping characters between chunks\n",
    ")\n",
    "\n",
    "pdf_doc_chunks=text_splitter.split_documents(pdf_data)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76fb9e7",
   "metadata": {},
   "source": [
    "Let's check out the distribution of chunk sizes.\n",
    "\n",
    "Run the cell below.\n",
    "\n",
    "Remember the document object should be called ```pdf_doc_chunks```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36390a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [len(doc.page_content) for doc in pdf_doc_chunks]\n",
    "\n",
    "plt.boxplot(data)  \n",
    "plt.title('Box Plot of chunk lengths')  # Title \n",
    "plt.xlabel('Chunk Lengths')  # Label for x-axis\n",
    "plt.ylabel('Values')  # Label for y-axis\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"The median chunk lenght is : {round(np.median(data),2)}\")\n",
    "print(f\"The average chunk lenght is : {round(np.mean(data),2)}\")\n",
    "print(f\"The minimum chunk lenght is : {round(np.min(data),2)}\")\n",
    "print(f\"The max chunk lenght is : {round(np.max(data),2)}\")\n",
    "print(f\"The 75th percentile chunk length is : {round(np.percentile(data, 75),2)}\")\n",
    "print(f\"The 25th percentile chunk length is : {round(np.percentile(data, 25),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1cb02",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Data Transformation or Embeddings</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c736c58",
   "metadata": {},
   "source": [
    "#### __OpenAI Embeddings__\n",
    "\n",
    "OpenAI, the company behind ChatGPT and GPT series of Large Language Models also provide three Embeddings Models. \n",
    "\n",
    "1.\ttext-embedding-ada-002 was released in December 2022. It has a dimension of 1536 meaning that it converts text into a vector of 1536 dimensions.\n",
    "2.\ttext-embedding-3-small is the latest small embedding model of 1536 dimensions released in January 2024. The flexibility it provides over ada-002 model is that users can adjust the size of the dimensions according to their needs.\n",
    "3.\ttext-embedding-3-large is a large embedding model of 3072 dimensions released together with the text-embedding-3-small model. It is the best performing model released by OpenAI yet.\n",
    "\n",
    "\n",
    "OpenAI models are proprietary and can be accessed using the OpenAI API and are priced based on the number of input tokens for which embeddings are desired. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6a735",
   "metadata": {},
   "source": [
    "Note: You will need an __OpenAI API Key__ which can be obtained from [OpenAI](https://platform.openai.com/api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab2968",
   "metadata": {},
   "source": [
    "To initialize the __OpenAI client__, we need to pass the api key. There are many ways of doing it. \n",
    "\n",
    "__[Option 1] Creating a .env file for storing the API key and using it # Recommended__\n",
    "\n",
    "Install the __dotenv__ library\n",
    "\n",
    "_The dotenv library is a popular tool used in various programming languages, including Python and Node.js, to manage environment variables in development and deployment environments. It allows developers to load environment variables from a .env file into their application's environment._\n",
    "\n",
    "- Create a file named .env in the root directory of their project.\n",
    "- Inside the .env file, then define environment variables in the format VARIABLE_NAME=value. \n",
    "\n",
    "e.g.\n",
    "\n",
    "OPENAI_API_KEY=YOUR API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5febefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "if load_dotenv():\n",
    "    print(\"Success: .env file found with some environment variables\")\n",
    "else:\n",
    "    print(\"Caution: No environment variables found. Please create .env file in the root directory or add environment variables in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee68d6",
   "metadata": {},
   "source": [
    "We can also test if the key is valid or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "if api_key:\n",
    "    try:\n",
    "        client.models.list()\n",
    "        print(\"OPENAI_API_KEY is set and is valid\")\n",
    "    except openai.APIError as e:\n",
    "        print(f\"OpenAI API returned an API Error: {e}\")\n",
    "        pass\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "        pass\n",
    "    except openai.RateLimitError as e:\n",
    "        print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "        pass\n",
    "\n",
    "else:\n",
    "    print(\"Please set you OpenAI API key as an environment variable OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f0119",
   "metadata": {},
   "source": [
    "Now we will use the __OpenAIEmbeddings__ library from langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34cbb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenAIEmbeddings from the library\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"\n",
    "\n",
    "pdf_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "pdf_doc_embeddings=embeddings.embed_documents([chunk.page_content for chunk in pdf_doc_chunks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeabad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The lenght of the embeddings vector is {len(pdf_doc_embeddings[0])}\")\n",
    "print(f\"The embeddings object is an array of {len(pdf_doc_embeddings)} X {len(pdf_doc_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d842f",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Vector Storage</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e01887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "storage_file_path=\"./Memory\"\n",
    "storage_index_name=\"PDF_index\"\n",
    "\n",
    "index = faiss.IndexFlatIP(len(pdf_doc_embeddings[0]))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=pdf_doc_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cebba",
   "metadata": {},
   "source": [
    "We can also save the vector store in persistent memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save_local(folder_path=storage_file_path,index_name=storage_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359639ca",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">Generation Pipeline</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f43f95",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">1. Retrieval</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c34744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS vector store with safe deserialization\n",
    "vector_store = FAISS.load_local(folder_path=\"./Memory\",index_name=\"CWC_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Define a query\n",
    "query = \"Who won the world cup?\"\n",
    "\n",
    "# Perform similarity search\n",
    "retrieved_docs = vector_store.similarity_search(query, k=2)  # Get top 2 relevant chunks\n",
    "\n",
    "# Display results\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(textwrap.fill(f\"\\nRetrieved Chunk {i+1}:\\n{doc.page_content}\",width=100))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a4144",
   "metadata": {},
   "source": [
    "This is the most basic implementation of a retriever in the generation pipeline of a RAG-enabled system. This method of retrieval is enabled by embeddings. We used the text-embedding-3-small from OpenAI. FAISS calculated the similarity score based on these embeddings.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029fd599",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">2. Augmentation</span>\n",
    "\n",
    "The information fetched by the retriever should also be sent to the LLM in form of a natural language prompt. This process of combining the user query and the retrieved information is called augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d27eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_context=retrieved_docs[0].page_content + retrieved_docs[1].page_content\n",
    "\n",
    "# Creating the prompt\n",
    "augmented_prompt=f\"\"\"\n",
    "\n",
    "Given the context below answer the question.\n",
    "\n",
    "Question: {query} \n",
    "\n",
    "Context : {retrieved_context}\n",
    "\n",
    "Remember to answer only based on the context provided and not from any other source. \n",
    "\n",
    "If the question cannot be answered based on the provided context, say I don’t know.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(textwrap.fill(augmented_prompt,width=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35515d46",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7237a",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">3. Generation</span>\n",
    "\n",
    "Generation is the final step of this pipeline. While LLMs may be used in any of the previous steps in the pipeline, the generation step is completely reliant on the LLM. The most popular LLMs are the ones being developed by OpenAI, Anthropic, Meta, Google, Microsoft and Mistral amongst other developers. \n",
    "\n",
    "We have built a simple retriever using FAISS and OpenAI embeddings and, we created a simple augmented prompt. Now we will use OpenAI’s model, GPT-4o-mini, to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd82c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Set up LLM and embeddings\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "messages=[(\"human\",augmented_prompt)]\n",
    "\n",
    "ai_msg = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c4ab24",
   "metadata": {},
   "source": [
    "And there you have it. The response is rooted in the HTML document and based on the chunks retrieved from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d219dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#START YOUR CODE HERE\n",
    "\n",
    "# Load the FAISS vector store with safe deserialization\n",
    "vector_store = FAISS.load_local(folder_path=\"./Memory\",index_name=\"PDF_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Define a query\n",
    "query = \"How many paternity leaves can I avail\"\n",
    "\n",
    "# Perform similarity search to get top 2 relevant chunks\n",
    "retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "#END YOUR CODE HERE\n",
    "\n",
    "# Display results\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(textwrap.fill(f\"\\nRetrieved Chunk {i+1}:\\n{doc.page_content}\",width=100))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a16a8c",
   "metadata": {},
   "source": [
    "Now craft the augmented prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefd7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#START YOUR CODE HERE\n",
    "\n",
    "retrieved_context=retrieved_docs[0].page_content + retrieved_docs[1].page_content\n",
    "\n",
    "# Creating the prompt\n",
    "augmented_prompt=f\"\"\"\n",
    "\n",
    "Given the context below answer the question.\n",
    "\n",
    "Question: {query} \n",
    "\n",
    "Context : {retrieved_context}\n",
    "\n",
    "Remember to answer only based on the context provided and not from any other source. \n",
    "\n",
    "If the question cannot be answered based on the provided context, say I don’t know.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#END YOUR CODE HERE\n",
    "\n",
    "print(textwrap.fill(augmented_prompt,width=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f907ab",
   "metadata": {},
   "source": [
    "Finally, make the call to the LLM. Use OpenAI's __gpt-4o-mini__ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400727b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START YOUR CODE HERE\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None\n",
    ")\n",
    "\n",
    "messages=[(\"human\",augmented_prompt)]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "\n",
    "# END YOUR CODE HERE\n",
    "\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d9163",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">Congratulations!</span>\n",
    "For completing this introduction to RAG. I hope you had fun. For any queries, please get in touch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
