{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d0b448",
   "metadata": {},
   "source": [
    "\n",
    "# 🧠 CV Screening RAG Chatbot — Hands‑On\n",
    "\n",
    "This notebook walks you through building a **Retrieval‑Augmented Generation (RAG) chatbot** over a collection of resumes (CVs).  \n",
    "We will **manually upload CV in PDF** and build a private, local search system.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Goals\n",
    "- Let HR quickly **search candidate CV** for skills, experience, and qualifications.\n",
    "- Use **RAG (Retrieval-Augmented Generation)** with OpenAI models to ground chatbot answers in CV data.\n",
    "\n",
    "---\n",
    "\n",
    "⚠️ **Privacy Note**: CV text and your queries will be sent to OpenAI's API to generate responses. Avoid uploading sensitive or personal information unless you have permission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db96bc3",
   "metadata": {},
   "source": [
    "\n",
    "## 1️⃣ Setup & Imports\n",
    "\n",
    "We’ll use:\n",
    "- `PyPDF2` → to extract text from CV (PDFs)\n",
    "- `tiktoken` → for tokenization when chunking text\n",
    "- `faiss` → local vector database to store embeddings\n",
    "- `openai` → for embeddings + chat completions\n",
    "- `dotenv` → to load API keys from `.env`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17758e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"❌ OPENAI_API_KEY not set. Please add it to your .env file.\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"✅ OpenAI client ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fcf7e3",
   "metadata": {},
   "source": [
    "\n",
    "## 2️⃣ Upload & Parse CV (PDFs)\n",
    "\n",
    "We’ll read CV from a local folder (`./uploads`).  \n",
    "Each PDF will be converted into plain text for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UPLOAD_DIR = \"uploads\"\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Parse all resumes in uploads/\n",
    "resumes = {}\n",
    "for fname in os.listdir(UPLOAD_DIR):\n",
    "    if fname.lower().endswith(\".pdf\"):\n",
    "        path = os.path.join(UPLOAD_DIR, fname)\n",
    "        resumes[fname] = extract_text_from_pdf(path)\n",
    "\n",
    "print(f\"✅ Loaded {len(resumes)} resumes\")\n",
    "list(resumes.keys())[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56471b",
   "metadata": {},
   "source": [
    "\n",
    "## 3️⃣ Chunking CV Texts\n",
    "\n",
    "Why chunking?  \n",
    "- CV can be long. Embedding entire CV leads to poor retrieval.  \n",
    "- We split text into **manageable chunks** (300–400 tokens) with slight overlap.\n",
    "\n",
    "This helps us retrieve only the most relevant pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8338b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tiktoken\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 400, overlap: int = 60) -> List[str]:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = enc.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens - overlap):\n",
    "        chunk = tokens[i:i+max_tokens]\n",
    "        chunks.append(enc.decode(chunk))\n",
    "    return chunks\n",
    "\n",
    "# Example chunking on first resume\n",
    "sample_resume = list(resumes.values())[0]\n",
    "chunks = chunk_text(sample_resume)\n",
    "print(f\"First resume split into {len(chunks)} chunks\")\n",
    "chunks[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5481c88",
   "metadata": {},
   "source": [
    "\n",
    "## 4️⃣ Embeddings + FAISS Index\n",
    "\n",
    "We’ll convert each chunk into a **vector embedding** using OpenAI.  \n",
    "Then, we’ll store all vectors in a **FAISS index** for fast similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e8d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DocChunk:\n",
    "    doc_id: str\n",
    "    chunk_id: int\n",
    "    text: str\n",
    "\n",
    "all_chunks: List[DocChunk] = []\n",
    "for doc_name, text in resumes.items():\n",
    "    chunks = chunk_text(text)\n",
    "    for i, ch in enumerate(chunks):\n",
    "        all_chunks.append(DocChunk(doc_id=doc_name, chunk_id=i, text=ch))\n",
    "\n",
    "print(f\"Total chunks: {len(all_chunks)}\")\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    vectors = []\n",
    "    for i in range(0, len(texts), 50):\n",
    "        batch = texts[i:i+50]\n",
    "        resp = client.embeddings.create(model=\"text-embedding-3-small\", input=batch)\n",
    "        vectors.extend([d.embedding for d in resp.data])\n",
    "        time.sleep(0.5)  # be polite\n",
    "    return np.array(vectors).astype(\"float32\")\n",
    "\n",
    "# Embed all chunks\n",
    "texts = [c.text for c in all_chunks]\n",
    "embeddings = embed_texts(texts)\n",
    "\n",
    "# Build FAISS index\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"✅ FAISS index ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859422e0",
   "metadata": {},
   "source": [
    "\n",
    "## 5️⃣ Retrieval + Answer Generation (RAG)\n",
    "\n",
    "Workflow:\n",
    "1. Embed the user’s question\n",
    "2. Retrieve top‑k most similar chunks\n",
    "3. Send them along with the question to OpenAI for a **grounded answer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b14ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_index(query: str, k: int = 3):\n",
    "    q_emb = embed_texts([query])\n",
    "    D, I = index.search(q_emb, k)\n",
    "    return [(all_chunks[i], float(D[0][j])) for j, i in enumerate(I[0])]\n",
    "\n",
    "def answer_with_rag(query: str, k: int = 3) -> str:\n",
    "    results = search_index(query, k)\n",
    "    context = \"\\n---\\n\".join([f\"[{r.doc_id}#{r.chunk_id}] {r.text}\" for r, _ in results])\n",
    "    prompt = f\"\"\"You are an HR assistant.\n",
    "Use the following resume excerpts to answer the question:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer clearly, citing resume IDs like [filename#chunk].\"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# Try a sample HR query\n",
    "print(answer_with_rag(\"Who has experience with Python and machine learning?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea4230",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ Key Takeaways\n",
    "\n",
    "- RAG lets HR **search CV** using natural language (not just keywords).\n",
    "- **Chunking + embeddings** improves retrieval accuracy.\n",
    "- **FAISS** is fast and lightweight for local experiments.\n",
    "- With **Streamlit** or Docker, this can be demoed to non-technical HR teams easily.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Next Steps\n",
    "- Add filters (e.g., years of experience, location).\n",
    "- Use a managed vector DB (Pinecone, Chroma, pgvector) for scale.\n",
    "- Improve PDF parsing (layout, tables, OCR).  \n",
    "- Add guardrails for bias + privacy.\n",
    "\n",
    "🎉 You now have a working RAG chatbot over CV!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
